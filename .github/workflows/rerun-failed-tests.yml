name: Rerun Failed Tests

on:
  workflow_dispatch:
    inputs:
      test-type:
        description: 'Type of tests to run'
        required: true
        type: choice
        options:
          - unit
          - integration
          - all
        default: 'all'
      test-path:
        description: 'Specific test file or test path (e.g., test/bot/test_microbot.py::TestMicroBot::test_microbot_ro_mount). Leave empty to rerun last failed tests.'
        required: false
        type: string
      last-failed-only:
        description: 'Run only last failed tests (uses pytest --lf)'
        required: false
        type: boolean
        default: true

jobs:
  rerun-tests:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        test-type: ${{ fromJSON(inputs.test-type == 'all' && '["unit", "integration"]' || format('["{0}"]', inputs.test-type)) }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Set up Docker Buildx
        if: matrix.test-type == 'integration'
        uses: docker/setup-buildx-action@v3

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Cache pytest cache
        uses: actions/cache@v4
        with:
          path: .pytest_cache
          key: ${{ runner.os }}-pytest-${{ matrix.test-type }}-${{ hashFiles('test/**/*.py') }}
          restore-keys: |
            ${{ runner.os }}-pytest-${{ matrix.test-type }}-

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-mock pytest-asyncio

      - name: Install package in development mode
        run: |
          pip install -e .

      - name: Build Docker images for integration tests
        if: matrix.test-type == 'integration'
        run: |
          # Build the shell server image needed for Docker tests
          docker build -f src/microbots/environment/local_docker/image_builder/Dockerfile -t kavyasree261002/shell_server:latest .

      - name: Determine test arguments
        id: test-args
        run: |
          PYTEST_ARGS="-m '${{ matrix.test-type }}'"
          
          # Add specific test path if provided
          if [ -n "${{ inputs.test-path }}" ]; then
            PYTEST_ARGS="$PYTEST_ARGS ${{ inputs.test-path }}"
          fi
          
          # Add --lf flag if last-failed-only is true and no specific path
          if [ "${{ inputs.last-failed-only }}" = "true" ] && [ -z "${{ inputs.test-path }}" ]; then
            PYTEST_ARGS="--lf $PYTEST_ARGS"
          fi
          
          echo "pytest_args=$PYTEST_ARGS" >> $GITHUB_OUTPUT

      - name: Run ${{ matrix.test-type }} tests
        env:
          # OpenAI API Configuration
          OPEN_AI_KEY: ${{ secrets.OPEN_AI_KEY }}
          OPEN_AI_DEPLOYMENT_NAME: ${{ secrets.OPEN_AI_DEPLOYMENT_NAME }}
          OPEN_AI_END_POINT: ${{ secrets.OPEN_AI_END_POINT }}
          # Azure OpenAI API Configuration
          AZURE_OPENAI_API_VERSION: ${{ secrets.AZURE_OPENAI_API_VERSION }}
          AZURE_OPENAI_API_KEY: ${{ secrets.AZURE_OPENAI_API_KEY }}
          AZURE_OPENAI_ENDPOINT: ${{ secrets.AZURE_OPENAI_ENDPOINT }}
          BROWSER_USE_LLM_MODEL: "gpt-5"
          BROWSER_USE_LLM_TEMPERATURE: 1
        run: |
          python -m pytest ${{ steps.test-args.outputs.pytest_args }} \
            --cov=src \
            --cov-report=xml \
            --cov-report=term-missing \
            --cov-append \
            --junitxml=test-results-${{ matrix.test-type }}.xml \
            -v \
            -o log_cli=true \
            -o log_cli_level=DEBUG \
            -o log_cli_format="%(asctime)s [%(levelname)s] %(name)s: %(message)s" \
            -o log_cli_date_format="%Y-%m-%d %H:%M:%S" \
            -s

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: rerun-test-results-${{ matrix.test-type }}
          path: test-results-*.xml

      - name: Upload coverage reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: rerun-coverage-${{ matrix.test-type }}
          path: coverage.xml

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        if: always()
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          file: ./coverage.xml
          flags: ${{ matrix.test-type }},rerun
          name: codecov-rerun-${{ matrix.test-type }}
          fail_ci_if_error: false

  rerun-summary:
    runs-on: ubuntu-latest
    needs: [rerun-tests]
    if: always()
    steps:
      - name: Download all test results
        uses: actions/download-artifact@v4
        with:
          pattern: rerun-test-results-*
          merge-multiple: true

      - name: Test Summary
        if: always()
        run: |
          echo "## Rerun Test Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "| Test Type | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Tests | ${{ needs.rerun-tests.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
